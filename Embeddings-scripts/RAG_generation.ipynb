{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from pymilvus import connections, Collection\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models import Embeddings, ModelInference\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes, ModelTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env values\n",
    "load_dotenv(override=True)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "COS_API_KEY = os.getenv(\"COS_API_KEY\")\n",
    "WATSON_STUDIO_PROJECT_ID = os.getenv(\"WATSON_STUDIO_PROJECT_ID\")\n",
    "MILVUS_HOST = os.getenv(\"MILVUS_GRPC_HOST\")\n",
    "MILVUS_PORT = int(os.getenv(\"MILVUS_GRPC_PORT\"))\n",
    "MILVUS_CERT_PATH = os.getenv(\"MILVUS_CERT_PATH\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 16:36:00,610 - INFO - Initializing Watsonx credentials\n",
      "2025-03-30 16:36:00,611 - INFO - Loading embedding model\n",
      "2025-03-30 16:36:01,013 - INFO - Client successfully initialized\n",
      "2025-03-30 16:36:01,192 - INFO - Loading LLM (ModelInference)\n",
      "2025-03-30 16:36:01,791 - INFO - Client successfully initialized\n",
      "2025-03-30 16:36:03,155 - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-03-20&project_id=cd99c1e5-2311-4803-b050-680fe88d1a77&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200 \"HTTP/1.1 200 OK\"\n",
      "2025-03-30 16:36:03,199 - INFO - Successfully finished Get available foundation models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-03-20&project_id=cd99c1e5-2311-4803-b050-680fe88d1a77&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200'\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€ Watsonx Credentials â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "logging.info(\"Initializing Watsonx credentials\")\n",
    "client = Credentials(url=\"https://us-south.ml.cloud.ibm.com\", api_key=COS_API_KEY)\n",
    "\n",
    "logging.info(\"Loading embedding model\")\n",
    "embedding = Embeddings(\n",
    "    model_id=EmbeddingTypes.IBM_SLATE_30M_ENG,\n",
    "    project_id=WATSON_STUDIO_PROJECT_ID,\n",
    "    credentials=client\n",
    ")\n",
    "\n",
    "logging.info(\"Loading LLM (ModelInference)\")\n",
    "llm = ModelInference(\n",
    "    model_id=ModelTypes.GRANITE_13B_INSTRUCT_V2,   # ibm/granite-13b-instruct-v2\n",
    "    project_id=WATSON_STUDIO_PROJECT_ID,\n",
    "    credentials=client\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Connect to Milvus â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "connections.connect(\n",
    "    alias=\"default\",\n",
    "    host=MILVUS_HOST,\n",
    "    port=MILVUS_PORT,\n",
    "    user=\"ibmlhapikey\",\n",
    "    password=COS_API_KEY,\n",
    "    secure=True,\n",
    "    server_ca=MILVUS_CERT_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Generation Function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def generate_answer(query: str, collection_name: str = \"carbon_embeddings\", top_k: int = 5) -> str:\n",
    "    logging.info(f\"Generating answer for query: {query}\")\n",
    "\n",
    "    # Step 1: Embed the query\n",
    "    query_vector = embedding.embed_documents([query])[0]\n",
    "\n",
    "    # Step 2: Search Milvus\n",
    "    collection = Collection(name=collection_name)\n",
    "    collection.load()\n",
    "    search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "\n",
    "    results = collection.search(\n",
    "        data=[query_vector],\n",
    "        anns_field=\"embedding\",\n",
    "        param=search_params,\n",
    "        limit=top_k,\n",
    "        output_fields=[\"chunk_text\", \"file_name\"]\n",
    "    )\n",
    "\n",
    "    # Step 3: Gather context\n",
    "    context_chunks = []\n",
    "    for hit in results[0]:\n",
    "        chunk = hit.entity.get(\"chunk_text\", \"\")\n",
    "        context_chunks.append(chunk.strip())\n",
    "    context = \"\\n\".join(context_chunks)\n",
    "\n",
    "    logging.info(f\"Retrieved {len(context_chunks)} relevant chunks from Milvus.\")\n",
    "\n",
    "    # Step 4: Construct prompt\n",
    "    prompt = f\"\"\"Use the context below to answer the user's question as clearly and accurately as possible.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Step 5: Generate response from Watsonx LLM\n",
    "    response = llm.generate(prompt=prompt)\n",
    "    answer = response['results'][0]['generated_text']\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 16:36:03,483 - INFO - Generating answer for query: Soyabean\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¬ CarbonSense | Ask your sustainability question\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 16:36:04,708 - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/embeddings?version=2025-03-20 \"HTTP/1.1 200 OK\"\n",
      "2025-03-30 16:36:04,710 - INFO - Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/embeddings?version=2025-03-20'\n",
      "2025-03-30 16:36:04,963 - ERROR - Error during generation: Hit.get() takes 2 positional arguments but 3 were given\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ’¬ CarbonSense | Ask your sustainability question\")\n",
    "\n",
    "try:\n",
    "    query = \"Soyabean\".strip()\n",
    "    if query.lower() == \"exit\":\n",
    "        pass\n",
    "\n",
    "    answer = generate_answer(query)\n",
    "    print(\"\\nðŸ§  Answer:\\n\", answer)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during generation: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
