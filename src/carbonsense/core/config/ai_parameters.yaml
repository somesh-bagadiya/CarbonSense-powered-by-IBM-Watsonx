###############################################
# CarbonSense AI Parameters Configuration
###############################################

default:
  # Core model parameters
  model: "watsonx/ibm/granite-3-3-8b-instruct"  # Default model for all agents
  # model: "watsonx/meta-llama/llama-3-3-70b-instruct"
  # model: "watsonx/meta-llama/llama-3-2-3b-instruct"
  temperature: 0.2                       # Controls randomness (0-1)
  top_p: 0.2                              # Nucleus sampling parameter
  # n: 1                                    # Number of completions
  max_tokens: 256                        # Maximum tokens in completion
  # presence_penalty: 0.0                   # Penalty for repeated topics (-2 to 2)
  # frequency_penalty: 0.0                  # Penalty for repeated tokens (-2 to 2)
  # logit_bias: {}                          # Token biasing dictionary
  # stop: []                               # List of stop sequences
  # stream: true                           # Enable streaming responses
  # timeout: 120                           # Request timeout in seconds
  
  # Advanced parameters
  # reasoning_effort: "medium"              # Options: none, low, medium, high
  # seed: null                             # Random seed for reproducibility
  # logprobs: null                         # Return log probabilities
  # top_logprobs: null                     # Number of most likely tokens
  cache_ttl_seconds: 86400

# Agent-specific parameters - only override what differs from defaults
agents:
  query_classifier:
    model: "watsonx/ibm/granite-3-3-8b-instruct"
    # model: "watsonx/meta-llama/llama-3-2-3b-instruct"
    temperature: 0.2
    reasoning_effort: "low"
    max_tokens: 256
    sources_tracked: false

  entity_extractor:
    model: "watsonx/ibm/granite-3-3-8b-instruct"
    # model: "watsonx/meta-llama/llama-3-2-3b-instruct"
    temperature: 0.2
    reasoning_effort: "low"
    max_tokens: 256
    sources_tracked: false

  unit_normalizer:
    model: "watsonx/ibm/granite-3-3-8b-instruct"
    # model: "watsonx/meta-llama/llama-3-2-3b-instruct"
    temperature: 0.1
    reasoning_effort: "low"
    max_tokens: 256
    sources_tracked: false

  milvus_researcher:
    model: "watsonx/ibm/granite-3-3-8b-instruct"
    # model: "watsonx/meta-llama/llama-3-2-3b-instruct"
    temperature: 0.2        # Lower temperature for more factual/precise responses
    reasoning_effort: "medium"  # Updated from "low" to "medium" for disambiguating semantic matches
    max_tokens: 512        # Larger context for research
    sources_tracked: true   # This agent tracks Milvus

  discovery_researcher:
    model: "watsonx/ibm/granite-3-3-8b-instruct"
    # model: "watsonx/meta-llama/llama-3-2-3b-instruct"
    temperature: 0.2
    reasoning_effort: "medium"  # Updated from "low" to "medium" for better source extraction
    max_tokens: 512
    sources_tracked: true

  serper_researcher:
    model: "watsonx/ibm/granite-3-3-8b-instruct"
    # model: "watsonx/meta-llama/llama-3-2-3b-instruct"
    temperature: 0.2
    reasoning_effort: "medium"
    max_tokens: 512
    sources_tracked: true

  carbon_estimator:
    model: "watsonx/ibm/granite-3-3-8b-instruct"
    # model: "watsonx/meta-llama/llama-3-2-3b-instruct"
    temperature: 0.2
    reasoning_effort: "medium"
    max_tokens: 512
    sources_tracked: true

  answer_consolidator:
    model: "watsonx/ibm/granite-3-3-8b-instruct"
    # model: "watsonx/meta-llama/llama-3-2-3b-instruct"
    temperature: 0.2
    reasoning_effort: "medium"  # Slightly higher for comparative evaluation
    max_tokens: 512
    sources_tracked: true

  unit_harmoniser:
    model: "watsonx/ibm/granite-3-3-8b-instruct"
    # model: "watsonx/meta-llama/llama-3-2-3b-instruct"
    temperature: 0.1
    reasoning_effort: "low"
    max_tokens: 256
    sources_tracked: false

  metric_ranker:
    model: "watsonx/ibm/granite-3-3-8b-instruct"
    # model: "watsonx/meta-llama/llama-3-2-3b-instruct"
    temperature: 0.1
    reasoning_effort: "low"
    max_tokens: 256
    sources_tracked: false

  comparison_formatter:
    model: "watsonx/ibm/granite-3-3-8b-instruct"
    # model: "watsonx/meta-llama/llama-3-2-3b-instruct"
    temperature: 0.2
    reasoning_effort: "medium"  # Updated from "low" to "medium" for future expansion
    max_tokens: 512
    sources_tracked: true

  recommendation_agent:
    model: "watsonx/ibm/granite-3-3-8b-instruct"
    # model: "watsonx/meta-llama/llama-3-2-3b-instruct"
    temperature: 0.2
    reasoning_effort: "medium"
    max_tokens: 512
    sources_tracked: true

  explanation_agent:
    model: "watsonx/ibm/granite-3-3-8b-instruct"
    # model: "watsonx/meta-llama/llama-3-2-3b-instruct"
    temperature: 0.2
    reasoning_effort: "medium"
    max_tokens: 512
    sources_tracked: true

  answer_formatter:
    model: "watsonx/ibm/granite-3-3-8b-instruct"
    # model: "watsonx/meta-llama/llama-3-2-3b-instruct"
    temperature: 0.2
    reasoning_effort: "medium"  # Updated from "low" to "medium" for lifecycle/compare handling
    max_tokens: 512
    sources_tracked: false
    
  footprint_cache:
    model: "watsonx/ibm/granite-3-3-8b-instruct"
    # model: "watsonx/meta-llama/llama-3-2-3b-instruct"
    temperature: 0.1
    reasoning_effort: "low"
    max_tokens: 256
    sources_tracked: false
    
  manager:
    # model: "watsonx/ibm/granite-3-3-8b-instruct"
    # model: "watsonx/meta-llama/llama-3-2-3b-instruct"
    model: "watsonx/meta-llama/llama-3-3-70b-instruct"
    temperature: 0.2        # Lower temperature for more precise task management
    reasoning_effort: "high"  # Higher reasoning for management
    max_tokens: 512        # Standard context size
    sources_tracked: false

# Response structure for the final compiler agent output
output_format:
  standard_json:
    emission: "Carbon footprint in kg CO2e (numeric value with units)"
    method: "Description of calculation methodology and data sources"
    category: "One of the five standard categories (Food & Diet, Energy Use, Mobility, Purchases, Miscellaneous)"
    sources: [
      {"title": "Source name", "url": "Source URL if available"}
    ]
  comparison_json:
    comparison_results: {
      "items": [
        {"product_name": "Product 1", "emission": "X kg CO2e", "category": "Category 1"},
        {"product_name": "Product 2", "emission": "Y kg CO2e", "category": "Category 2"}
      ],
      "difference": {
        "absolute": "Absolute difference in kg CO2e",
        "percentage": "Percentage difference",
        "better_option": "Lower carbon option"
      }
    }
    method: "Description of comparison methodology"
    sources: [
      {"title": "Source name", "url": "Source URL if available"}
    ]

# Data sources tracked
data_sources:
  milvus_database:
    description: "Vector database containing pre-processed carbon footprint data"
    collections:
      - "carbon_embeddings_30m"   # 30M model embeddings
      - "carbon_embeddings_125m"  # 125M model embeddings
      - "carbon_embeddings_granite" # Granite model embeddings (preferred)
  
  watson_discovery:
    description: "Watson Discovery service for document search and web queries"
    confidence_threshold: 0.6   # Minimum confidence score for web results
    max_results: 5             # Maximum number of search results to return
    
  serper_dev:
    description: "Serper.dev API for web search integration"
    confidence_threshold: 0.5  # Minimum confidence score
    max_results: 5             # Maximum number of search results