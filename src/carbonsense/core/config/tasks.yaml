# --------------------------------
# 1. QUERY UNDERSTANDING LAYER
query_classification_task:
  description: >
    Analyze the query "{query}" to determine both intent type and category.
    
    Return a JSON object with the following structure:
    {
      "query_intent": "estimate|compare|suggest|lifecycle|myth_bust",
      "category": "Food & Diet|Energy Use|Mobility|Purchases|Miscellaneous",
      "confidence": <float between 0-1 indicating classification confidence>
    }
    
    Intent types are defined as:
    - estimate: User wants to know the carbon footprint of a specific product/activity
    - compare: User wants to compare carbon footprints of two or more products/activities
    - suggest: User wants recommendations for reducing their carbon footprint
    - lifecycle: User wants to understand the full lifecycle emissions of a product
    - myth_bust: User wants to verify a claim or common belief about carbon footprints
    
    Categories are defined as:
    - Food & Diet: All food items, beverages, ingredients, agricultural products
    - Energy Use: Electricity, heating, cooling, appliances, home energy consumption
    - Mobility: Transportation, vehicles, flights, public transit, commuting
    - Purchases: Consumer goods, electronics, clothing, services, streaming
    - Miscellaneous: Mixed activities, lifestyle changes, or items that don't fit other categories
    
    Important guidelines:
    - Analyze both explicit and implicit intent in the query
    - For ambiguous queries, select the most likely intent based on context
    - If multiple intents are present, choose the dominant one
    - Assign a confidence score that reflects your certainty in the classification
    - For mixed category queries, choose the most prominent category
  expected_output: >
    A JSON object containing the query intent, category classification, and confidence score.
  agent: query_classifier
  output_file: logs/1_classification.json
  async_execution: false

entity_extraction_task:
  description: >
    Extract key entities from the query "{query}" based on the identified intent and category.
    
    Return a JSON object with the following structure for a standard query:
    {
      "products": [{"name": "product name", "quantity": number, "unit": "kg|km|kWh|item"}],
      "location": "location name or null if none specified",
      "timeframe": "timeframe or null if none specified",
      "additional_context": "any other relevant context extracted"
    }
    
    For comparison queries, include multiple products in the products array.
    
    Important guidelines:
    - Extract all products/activities mentioned in the query
    - Include quantities and units when specified
    - If no quantity is specified, don't include a quantity field
    - Extract location information when present (country, region, city)
    - Identify timeframes or durations when mentioned
    - Capture any additional context that might be relevant for calculation
    - Format the response as a proper JSON object
  expected_output: >
    A JSON object containing extracted entities including products, quantities, locations, and other context.
  agent: entity_extractor
  output_file: logs/2_entities.json
  async_execution: false
  context: [query_classification_task]

unit_normalization_task:
  description: >
    Normalize all extracted entities with their quantities into standard units based on the entity type.
    Use the entity extraction results and apply consistent unit conversion and standardization.
    
    Return a JSON object with the following structure:
    {
      "normalized_products": [
        {
          "name": "standardized product name",
          "raw_quantity": "original quantity text or null",
          "standard_quantity": {
            "value": <float - never zero unless explicitly specified, default to 1.0 for missing quantities>,
            "unit": "kg|km|kWh|item - standardized unit appropriate for the product type"
          },
          "quantity_missing": <boolean - true if quantity was not specified in the query>
        }
      ],
      "location": "standardized location name or null",
      "timeframe": {
        "value": <float - duration value or null>,
        "unit": "hours|days|years or null",
        "standardized": <boolean - whether the timeframe was standardized>
      }
    }
    
    Important guidelines:
    - Convert all units to standard units based on entity type:
      * Food: kg (convert pounds, ounces, grams, etc.)
      * Transportation: km (convert miles, feet, meters, etc.)
      * Energy: kWh (convert joules, BTU, etc.)
      * Items: each/item (count of individual objects)
    - Apply logical defaults for missing quantities:
      * Food items: 1 kg (or typical serving size if known)
      * Discrete objects: 1 item
      * Transportation: 1 km
      * Energy sources: 1 kWh
    - Never return zero quantities unless explicitly specified
    - Normalize product names to their canonical form where appropriate
    - Standardize location names to consistent formats
    - Convert timeframes to standard units (hours, days, years)
  expected_output: >
    A JSON object containing normalized entities with standardized units and appropriate defaults.
  agent: unit_normalizer
  output_file: logs/3_normalized.json
  async_execution: false
  context: [entity_extraction_task, query_classification_task]

# 2. CACHE LOOKUP (SKIP RESEARCH WHEN HIT)
cache_lookup_task:
  description: >
    Check the file cache for the products identified in the entity extraction.
    Use the normalized product names from the unit_normalization_task as keys.
    
    For each product, return either the cached metric or a MISS status.
    Return an array of results with the following structure:
    [
      {
        "product_name": "normalized product name",
        "status": "HIT|MISS",
        "data": {<full carbon metric object>} or null if MISS
      }
    ]
    
    Important guidelines:
    - Check each product separately for cache hits
    - For cache hits, include the complete carbon metric with all required fields
    - For cache misses, set status to "MISS" and data to null
    - For comparison queries, return an array with results for all products
    - Ensure all returned data follows the required schema
    - Never return plain text responses; always use proper JSON format
  expected_output: >
    An array of JSON objects, each containing either a complete cached carbon metric
    or a MISS status for each product in the query.
  agent: footprint_cache
  output_file: logs/4_cache.json
  async_execution: false
  context: [unit_normalization_task]

# 3. EXTERNAL RESEARCH (parallel)
milvus_research_task:
  description: >
    Query the Milvus vector database to find carbon footprint metrics for the products identified in the normalization task.
    For products with cache hits, skip the research.
    
    Return an array of complete JSON objects that follow the required schema:
    [
      {
        "product_name": "exact product name as extracted",
        "value": <number - precise carbon footprint value, never zero or negative>,
        "emission_unit": "kg CO2e" or "g CO2e",
        "product_unit": "per kg" or appropriate unit,
        "source": "milvus" plus any specific source document,
        "confidence": <number between 0-1 indicating data reliability>,
        "category": <one of: "Food & Diet", "Energy Use", "Mobility", "Purchases", "Miscellaneous">
      }
    ]
    
    Important guidelines:
    - Research each product that had a cache MISS
    - Use semantic search to find close matches even when product names aren't exact
    - Include the exact product_name for traceability
    - Assign confidence scores based on match quality and data source reliability
    - Never return zero or negative values unless they represent carbon sequestration
    - Categorize products according to the five standard categories
    - Handle special cases like carbon-negative products appropriately
    - If multiple matches are found, select the most relevant and accurate one
  expected_output: >
    An array of JSON objects with carbon footprint data following the common schema,
    including value, emission_unit, product_unit, source, confidence, product_name, and category.
    The data should be high quality, accurate, and logically consistent.
  agent: milvus_researcher
  output_file: logs/5_milvus.json
  async_execution: true
  context: [unit_normalization_task, cache_lookup_task, query_classification_task]

discovery_research_task:
  description: >
    Search Watson Discovery for carbon footprint information about the products identified in the normalization task.
    For products with cache hits, skip the research.
    
    Return an array of complete JSON objects that follow the required schema:
    [
      {
        "product_name": "exact product name as extracted",
        "value": <number - precise carbon footprint value, never zero or negative unless appropriate>,
        "emission_unit": "kg CO2e" or "g CO2e",
        "product_unit": "per kg" or appropriate unit,
        "source": specific source document or database from Discovery,
        "confidence": <number between 0-1 indicating data reliability>,
        "category": <one of: "Food & Diet", "Energy Use", "Mobility", "Purchases", "Miscellaneous">
      }
    ]
    
    Important guidelines:
    - Research each product that had a cache MISS
    - Extract data from the most authoritative sources available (academic > government > industry)
    - Prefer recent data (published within last 3 years) over older information
    - Calculate confidence scores based on source credibility, methodology, and data recency
    - Identify the specific document or database the data came from
    - Verify units are clearly specified and convert to standard units when necessary
    - Ensure all required fields are included and properly formatted
    - Categorize the product into one of the five standard categories
  expected_output: >
    An array of JSON objects with carbon footprint data following the common schema,
    including value, emission_unit, product_unit, source, confidence, product_name, and category.
    The data should be traced to specific documents and include appropriate confidence scoring.
  agent: discovery_researcher
  output_file: logs/6_discovery.json
  async_execution: true
  context: [unit_normalization_task, cache_lookup_task, query_classification_task]

serper_research_task:
  description: >
    Search the web using Serper for carbon footprint data about the products identified in the normalization task.
    For products with cache hits, skip the research.
    
    Return an array of complete JSON objects that follow the required schema:
    [
      {
        "product_name": "exact product name as extracted",
        "value": <number - precise carbon footprint value, never zero or negative unless appropriate>,
        "emission_unit": "kg CO2e" or "g CO2e",
        "product_unit": "per kg" or appropriate unit,
        "source": specific website, organization, or publication name,
        "confidence": <number between 0-1 indicating data reliability>,
        "category": <one of: "Food & Diet", "Energy Use", "Mobility", "Purchases", "Miscellaneous">
      }
    ]
    
    Important guidelines:
    - Research each product that had a cache MISS
    - Prioritize data from reputable sources (academic journals, government agencies, respected environmental organizations)
    - Verify the methodology used to calculate the carbon footprint when possible
    - Assign confidence scores based on source credibility, methodology transparency, and data recency
    - Extract the specific website, organization, or publication name for the source field
    - Make sure to include all required fields in your response
    - Handle special cases, regional variations, and different production methods
    - Categorize the product according to the standardized categories
  expected_output: >
    An array of JSON objects with carbon footprint data following the common schema,
    including value, emission_unit, product_unit, source, confidence, product_name, and category.
    The data should come from reputable online sources with clear attribution.
  agent: serper_researcher
  output_file: logs/7_serper.json
  async_execution: true
  context: [unit_normalization_task, cache_lookup_task, query_classification_task]

# 4. HARMONISE & ESTIMATE
harmonise_task:
  description: >
    Harmonize all carbon footprint metrics from the research tasks to ensure consistent units and formats.
    Convert all metrics to a standardized format (kg CO2e per kg or appropriate unit) for fair comparison.
    
    For each product, combine metrics from all sources, then:
    
    1. Convert all emission units to kg CO2e (e.g., g CO2e ÷ 1000)
    2. Standardize product units to "per kg" for food, "per km" for transport, etc.
    3. For non-weight items, establish reasonable weight equivalents when needed
    4. Ensure consistent precision and significant figures
    5. Verify all metrics contain complete information including product_name and category
    6. Validate category assignment and correct if necessary
    
    Return an array of arrays, where each inner array contains the harmonized metrics for one product:
    [
      [  // Metrics for product 1
        {
          "product_name": "product 1 name",
          "value": <number - normalized to standard units>,
          "emission_unit": "kg CO2e",
          "product_unit": "kg|km|kWh|item",
          "source": <original source>,
          "confidence": <original confidence score>,
          "category": <one of the five standard categories>
        },
        {...}  // Additional metrics for product 1 from different sources
      ],
      [  // Metrics for product 2 (if applicable)
        {...},
        {...}
      ]
    ]
    
    Important guidelines:
    - Group metrics by product name
    - Handle edge cases like very small or very large values appropriately
    - Never lose precision by inappropriate rounding (especially for small values)
    - Document any conversion factors or assumptions used
    - Preserve the original source and confidence information
    - Ensure all harmonized metrics are logically consistent
    - Validate and standardize category assignments across all metrics
  expected_output: >
    An array of arrays containing harmonized carbon footprint metrics,
    all with consistent units and formatting including product_name and category.
    The array should maintain all relevant information from the original metrics
    while ensuring they can be fairly compared.
  agent: unit_harmoniser
  output_file: logs/8_harmonised.json
  async_execution: false
  context: [milvus_research_task, discovery_research_task, serper_research_task, unit_normalization_task, query_classification_task]

carbon_estimation_task:
  description: >
    Calculate precise carbon footprints for each product using the harmonized metrics and normalized quantities.
    For queries with intent "lifecycle", include full lifecycle considerations.
    For queries with regional specifications, adjust calculations accordingly.
    
    Return an array of calculated carbon footprints with the following structure:
    [
      {
        "product_name": "product name",
        "total_emission": <number - calculated total emissions>,
        "emission_unit": "kg CO2e",
        "base_value": <number - base emission factor used>,
        "base_unit": "kg CO2e per kg|km|kWh|item",
        "quantity": <number - quantity used in calculation>,
        "quantity_unit": "kg|km|kWh|item",
        "source": "source of the base emission factor",
        "confidence": <number between 0-1>,
        "category": <one of the five standard categories>,
        "calculation_method": "description of calculation method used",
        "lifecycle_stages": ["production", "use", "disposal"] // Only for lifecycle queries
      }
    ]
    
    Important guidelines:
    - Multiply the base emission factor by the normalized quantity
    - For lifecycle analyses, include emissions from all relevant lifecycle stages
    - For regional calculations, adjust emission factors based on location-specific data
    - Document the calculation methodology and any assumptions made
    - Ensure results are logically consistent and plausible
    - Include confidence scores that reflect certainty in the calculation
    - Preserve category assignments from the harmonized metrics
  expected_output: >
    An array of JSON objects containing calculated carbon footprints for each product,
    including total emissions, calculation methodology, and all required metadata.
  agent: carbon_estimator
  output_file: logs/9_estimation.json
  async_execution: false
  context: [harmonise_task, unit_normalization_task, query_classification_task]

rank_metrics_task:
  description: >
    Evaluate and rank all carbon footprint estimates to identify the most reliable and accurate data point for each product.
    
    For each product, apply a weighted scoring system based on multiple factors:
    
    1. Source credibility: academic (1.0) > government (0.9) > industry (0.8) > general web (0.7)
    2. Methodological completeness: full LCA (1.0) > partial assessment (0.8) > estimation (0.6)
    3. Data recency: current year (1.0) with exponential decay for older data
    4. Original confidence scores provided by research agents
    5. Statistical robustness and precision of the data
    
    Calculate a 95% confidence interval when sufficient data is available.
    
    Return an array of JSON objects, one for each product, with the best metrics:
    [
      {
        "product_name": "product name",
        "value": <number - the best carbon footprint value>,
        "emission_unit": "kg CO2e",
        "product_unit": "kg|km|kWh|item",
        "source": <source of the selected metric>,
        "confidence": <final calculated confidence score>,
        "category": <one of the five standard categories>,
        "confidence_interval": "<lower bound> - <upper bound>",
        "recency": "<year of data or 'latest'>"
      }
    ]
    
    Important guidelines:
    - When metrics disagree significantly, investigate why and exclude obvious outliers
    - Consider the consensus of multiple sources when available
    - Never select metrics with logical inconsistencies or impossible values
    - Calculate a composite confidence score that reflects overall data quality
    - Always include the product_name field matched to the original query
    - Verify the category assignment is correct based on the product type
    - Provide a confidence interval when possible to communicate uncertainty
  expected_output: >
    An array of JSON objects representing the best carbon footprint metric for each product,
    selected based on a rigorous evaluation of confidence, reliability, recency, and source quality.
    Each object should include the standard fields plus confidence_interval and recency when available.
  agent: metric_ranker
  output_file: logs/10_ranked.json
  async_execution: false
  context: [carbon_estimation_task, query_classification_task]

# 5. INTENT-SPECIFIC PROCESSING
comparison_task:
  description: >
    Create a clear comparison between multiple carbon footprints when the query intent is "compare".
    Skip this task if the query intent is not "compare".
    
    Return a JSON object with the following structure:
    {
      "comparison_results": {
        "items": [
          {
            "product_name": "product 1 name",
            "emission": "12.5 kg CO₂e",
            "category": "category 1"
          },
          {
            "product_name": "product 2 name",
            "emission": "8.3 kg CO₂e",
            "category": "category 2"
          }
        ],
        "difference": {
          "absolute": "4.2 kg CO₂e",
          "percentage": "33.6% less",
          "better_option": "product 2 name"
        },
        "equivalent": "This difference is equivalent to driving a car for X km"
      },
      "method": "Description of the comparison methodology and data sources",
      "sources": [
        {"title": "Source 1", "url": "https://example.com/..."},
        {"title": "Source 2", "url": "https://example.org/..."}
      ]
    }
    
    Important guidelines:
    - Only process comparison queries, skip for other intents
    - Calculate absolute and percentage differences between items
    - Identify the lower-carbon option
    - Provide real-world equivalents to contextualize the difference
    - Include methods and sources used for all items in the comparison
    - Ensure fair comparison by using consistent units and methodologies
    - Format the output in the specified JSON structure
  expected_output: >
    A JSON object containing structured comparison results with emissions for each item,
    difference calculations, the better option, equivalent activities, methodology, and sources.
  agent: comparison_formatter
  output_file: logs/11_comparison.json
  async_execution: false
  context: [rank_metrics_task, query_classification_task]

recommendation_task:
  description: >
    Generate evidence-based recommendations for reducing carbon footprints when query intent is "suggest".
    Skip this task if the query intent is not "suggest".
    
    Return a JSON object with the following structure:
    {
      "recommendations": [
        {
          "action": "Specific recommendation",
          "potential_saving": "Estimated carbon saving",
          "feasibility": "High|Medium|Low",
          "cost_implication": "Cost impact description",
          "co_benefits": ["Health benefit", "Financial benefit", "etc."]
        }
      ],
      "category": "The category from the query (Food & Diet, Energy Use, etc.)",
      "baseline": "Current carbon footprint if available",
      "method": "Description of recommendation methodology",
      "sources": [
        {"title": "Source 1", "url": "https://example.com/..."},
        {"title": "Source 2", "url": "https://example.org/..."}
      ]
    }
    
    Important guidelines:
    - Only process suggestion queries, skip for other intents
    - Provide specific, actionable recommendations
    - Quantify carbon savings whenever possible
    - Assess feasibility and cost implications
    - Highlight co-benefits beyond carbon reduction
    - Base recommendations on scientific evidence and reliable sources
    - Focus on the category identified in the query classification
    - Format the output in the specified JSON structure
  expected_output: >
    A JSON object containing structured recommendations with specific actions,
    potential carbon savings, feasibility, costs, co-benefits, methodology, and sources.
  agent: recommendation_agent
  output_file: logs/12_recommendation.json
  async_execution: false
  context: [rank_metrics_task, query_classification_task, carbon_estimation_task]

explanation_task:
  description: >
    Provide clear, scientifically accurate explanations of carbon footprint concepts or debunk
    common misconceptions when query intent is "myth_bust".
    Skip this task if the query intent is not "myth_bust".
    
    Return a JSON object with the following structure:
    {
      "explanation": {
        "claim": "The claim or misconception being addressed",
        "verdict": "True|Mostly True|Partially True|Mostly False|False",
        "explanation": "Detailed explanation with evidence",
        "nuances": "Important nuances or context"
      },
      "category": "The category from the query",
      "method": "Description of fact-checking methodology",
      "sources": [
        {"title": "Source 1", "url": "https://example.com/..."},
        {"title": "Source 2", "url": "https://example.org/..."}
      ]
    }
    
    Important guidelines:
    - Only process myth-busting queries, skip for other intents
    - Clearly identify the claim being evaluated
    - Provide a nuanced verdict rather than simple true/false when appropriate
    - Include detailed explanation with scientific evidence
    - Acknowledge uncertainties and context that affect the answer
    - Use reputable, peer-reviewed sources whenever possible
    - Maintain scientific accuracy while being accessible
    - Format the output in the specified JSON structure
  expected_output: >
    A JSON object containing a structured explanation with the claim, verdict,
    detailed explanation, important nuances, methodology, and sources.
  agent: explanation_agent
  output_file: logs/13_explanation.json
  async_execution: false
  context: [rank_metrics_task, query_classification_task, carbon_estimation_task]

# 6. FINAL FORMATTING
answer_formatting_task:
  description: >
    Format the final answer for the user based on all collected data and analyses.
    Ensure the response is structured as a valid JSON object with all required fields.
    
    Return a JSON object with the following structure:
    {
      "answer": "A clear, direct answer to the user's question",
      "method": "Brief explanation of how this answer was calculated or determined",
      "confidence": <number between 0.0-1.0>,
      "category": "Food & Diet|Energy Use|Mobility|Purchases|Miscellaneous"
    }
    
    Important guidelines for determining confidence score:
    - 0.9-1.0: High-quality direct data from trusted academic or government sources
    - 0.7-0.89: Good information with minor extrapolation or assumptions
    - 0.5-0.69: Educated estimate based on similar cases or mixed-quality sources
    - 0.3-0.49: General approximation with limited data points
    - Below 0.3: Very limited data, significant assumptions required
    
    Important guidelines for determining category:
    - Food & Diet: All food items, beverages, agricultural products, dietary choices
    - Energy Use: Electricity, heating, cooling, appliances, home energy consumption
    - Mobility: Transportation, vehicles, flights, public transit, commuting
    - Purchases: Consumer goods, electronics, clothing, services, streaming
    - Miscellaneous: Mixed activities, lifestyle changes, or items that don't fit other categories
    
    Your output MUST be a valid JSON object with these exact keys and no additional text.
    DO NOT include any markdown formatting, explanation text, or code block syntax.
    The output should be parseable directly as JSON without any pre-processing.
    
    For example, output SHOULD look like this:
    {"answer": "The carbon footprint is 2.5 kg CO2e", "method": "Based on scientific data", "confidence": 0.85, "category": "Food & Diet"}
    
    Output should NOT look like this:
    ```json
    {"answer": "The carbon footprint is 2.5 kg CO2e", "method": "Based on scientific data", "confidence": 0.85, "category": "Food & Diet"}
    ```
    
    The crew_agent.py process_output method will attempt to extract JSON from your response, but providing clean, 
    properly formatted JSON without any surrounding text will ensure the most reliable processing.
  expected_output: >
    A properly formatted JSON object containing the answer, method, confidence score,
    and category that directly responds to the user's query, with no additional text or formatting.
  agent: answer_formatter
  output_file: logs/14_formatted_answer.json
  async_execution: false
  context: [query_classification_task, rank_metrics_task, comparison_task, recommendation_task, explanation_task, carbon_estimation_task]
